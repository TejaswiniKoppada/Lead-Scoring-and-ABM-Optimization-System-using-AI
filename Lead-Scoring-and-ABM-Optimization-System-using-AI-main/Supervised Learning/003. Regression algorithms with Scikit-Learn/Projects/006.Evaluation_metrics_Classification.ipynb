{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "Evaluation metrics are used to measure the quality of the model. One of the most important topics in machine learning is how to evaluate your model. When you build your model, it is very crucial to measure how accurately it predicts your expected outcome.\n",
    "\n",
    "In this lab, we will cover Evaluation metrics for Classification. There are many ways to measure classification performance: Accuracy, and confusion matrices, are some of the most popular metrics. Precision & recall are widely used metrics for classification problems.\n",
    "\n",
    "Let's start with some definitions:\n",
    "\n",
    "#### Confusion Matrix: \n",
    "A confusion matrix is a table that summarizes the performance of a classifier by showing the number of true positive, true negative, false positive, and false negative predictions. It is useful to understand the strengths and weaknesses of a classifier, and also provides information about the distribution of errors made by the classifier:\n",
    "\n",
    "    - TP (True Positive)**- The model correctly predicted the positive class.\n",
    "    **- TN (True Negative)** - The model correctly predicted the negative class.\n",
    "    **- FP (False Positive)** - The model predicted the positive class, but it's actually negative.\n",
    "    **- FN (False Negative)** - The model predicted the negative class, but it's actually positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy: \n",
    "This metric is defined as the ratio of correctly predicted instances to the total number of instances in the dataset. It measures how often the classifier is correct. However, it can be misleading in imbalanced datasets, as it may give high accuracy scores even if the classifier is not able to accurately predict the minority class. Based on the confusion matrix shows, the accuracy is computed as:\n",
    "        TP+TN/TP+TN+FP+FN\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision: \n",
    "Precision is the ratio of correctly predicted positive instances to the total number of instances predicted as positive. It is a measure of the classifier's ability to correctly identify positive instances and avoid false positives.\n",
    "        TP/TP+FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall (or Sensitivity): \n",
    "Recall is the ratio of correctly predicted positive instances to the total number of actual positive instances. It is a measure of the classifier's ability to detect all positive instances.\n",
    "        TP/TP+FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score: \n",
    "The F1 Score is the harmonic mean of precision and recall. It balances precision and recall and gives an overall performance score for the classifier.\n",
    "2 xPrecision x recall/precision+recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, TN, FP, FN = 4, 91, 1, 4\n",
    "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n"
     ]
    }
   ],
   "source": [
    "TP, TN, FP, FN = 0, 95, 5, 0\n",
    "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.890625\n"
     ]
    }
   ],
   "source": [
    "TP, FP = 114, 14\n",
    "precision=TP/(TP+FP)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "TP, FN = 114, 0\n",
    "recall=TP/(TP+FN)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.042105263157894736\n"
     ]
    }
   ],
   "source": [
    "TP, FP, FN = 2.00,  1.00, 90.00\n",
    "\n",
    "Precision=TP/(TP+FP)\n",
    "Recall=TP/(TP+FN)\n",
    "\n",
    "f1_score=2*Precision*Recall/(Precision+Recall)\n",
    "print(f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0]\n",
      " [1 3]]\n",
      "Accuracy:  0.8333333333333334\n",
      "Precision:  1.0\n",
      "Recall:  0.75\n",
      "F1-score:  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 1]\n",
    "y_pred = [1, 0, 1, 0, 0, 1]\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred)\n",
    "rec = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(conf_matrix)\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", rec)\n",
    "print(\"F1-score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6\n",
      "Precision:  0.6\n",
      "Recall:  0.6\n",
      "F1 Score:  0.6\n",
      "Confusion Matrix: \n",
      " [[3 2]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# True labels of the data\n",
    "y_true = [0, 1, 0, 1, 1, 0, 1, 1, 0, 0]\n",
    "\n",
    "# Predicted labels of the data\n",
    "y_pred = [0, 1, 0, 1, 0, 1, 1, 0, 1, 0]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(\"Precision: \", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(\"F1 Score: \", f1)\n",
    "\n",
    "# Calculate Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix: \\n\", conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[4,2,1],[3,4,6],[5,6,7],[8,9,7]]\n",
    "y_train = [1,2,1,2]\n",
    "X_test = [[4,3,1],[2,4,3],[5,6,1],[5,9,9]]\n",
    "y_test = [1,2,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "# Train the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=0)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the logistic regression model\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate the evaluation metrics for logistic regression\n",
    "prec_log_reg = precision_score(y_test, y_pred_log_reg, average=\"weighted\")\n",
    "\n",
    "# Print the evaluation metrics for logistic regression\n",
    "print(\"Precision: \", prec_log_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[4,2,1],[3,4,6],[5,6,7],[8,9,7]]\n",
    "y_train = [1,2,1,2]\n",
    "X_test = [[4,3,1],[2,4,3],[5,6,1],[5,9,9]]\n",
    "y_test = [1,2,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import  recall_score\n",
    "\n",
    "# Train the decision tree model\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the decision tree model\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Calculate the evaluation metrics for decision tree\n",
    "rec_dt = recall_score(y_test, y_pred_dt, average=\"weighted\")\n",
    "\n",
    "# Print the evaluation metrics for logistic regression\n",
    "print(\"Recall: \", rec_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit a Decision tree and a Logistic Regression models using the given input and output patterns X and Y, respectively. Once fitted, determine which models presents the best performance in term of F1-score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_train, y_train = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=2.3)\n",
    "\n",
    "X_test, y_test = make_blobs(n_samples=10, centers=2,\n",
    "                  random_state=0, cluster_std=4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT F1 Score: 0.8\n",
      "LR F1 Score: 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Train the decision tree model\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the decision tree model\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_dt=f1_score(y_test, y_pred_dt)\n",
    "print(\"DT F1 Score:\", f1_dt)\n",
    "\n",
    "lr=LogisticRegression(random_state=0)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr=lr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_lr=f1_score(y_test, y_pred_lr)\n",
    "print(\"LR F1 Score:\",f1_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
